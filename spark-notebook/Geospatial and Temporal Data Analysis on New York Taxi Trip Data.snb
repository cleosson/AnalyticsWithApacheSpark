{"metadata":{"name":"Geospatial and Temporal Data Analysis on New York Taxi Trip Data","user_save_timestamp":"1969-12-31T21:00:00.000Z","auto_save_timestamp":"1969-12-31T21:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":"/tmp/repo-test","customRepos":null,"customDeps":["org.bdgenomics.adam % adam-apis % 0.16.0","- org.apache.hadoop % hadoop-client %   _","+ org.apache.spark  %  spark-mllib_2.10  % 1.2.1"],"customImports":["import scala.util._","import org.apache.spark.SparkContext._"],"customArgs":null,"customSparkConf":{"spark.master":"local[4]","spark.driver.memory":"8G","spark.app.name":"Analytics with Apache Spark"}},"cells":[{"metadata":{"presentation":{"cell_width":"12"},"id":"D7D1B83DA76C4F09B830109A6F9E97DB"},"cell_type":"markdown","source":"###We are going to load the New York Taxi Trip Data for Yellow Cabs from year 2015.  \n\n###For demonstration purposes, we are going to get the data from January 2015.  \n\n###site: http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"9DBC12D77CCB46F09AECA307651F56C0"},"cell_type":"code","source":"val path = sys.env(\"AAS_DATA_PATH\")\nval csvPath = sys.env(\"AAS_CSV_FILE_PATH\")\n\nval taxiRaw = sparkSession.read.option(\"header\", \"true\").csv(csvPath).sample(false, 0.1d)\n\n","outputs":[]},{"metadata":{"id":"7BDA88C83E1542F282C9F24511E90916"},"cell_type":"markdown","source":"###*sparkSession.read.option().csv()* creates a DataFrame.  \n\n###DataFrame is a Dataset[org.apache.spark.sql.Row]\n\n###Let's see the fields that we have here"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab1429020406-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"F427F010C76C42D2808F508869D18686"},"cell_type":"code","source":"taxiRaw.take(4)","outputs":[]},{"metadata":{"id":"204D36B1437F46BB862D2F5349314C22"},"cell_type":"markdown","source":"###We can see the dictionary of the data here http://www.nyc.gov/html/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\n\n###The important fileds for us are:  \n\n| Field  | Description  |\n| ------ | ------------ |\n|tpep_pickup_datetime | The date and time when the meter was engaged |\n|tpep_dropoff_datetime | The date and time when the meter was disengaged |\n|passenger_count | The number of passengers in the vehicle |\n|trip_distance | The elapsed trip distance in miles reported by the taximeter |\n|pickup_longitude | Longitude where the meter was engaged |\n|pickup_latitude | Latitude where the meter was engaged |\n|dropoff_longitude | Longitude where the meter was disengaged |\n|dropoff_latitude | Latitude where the meter was disengaged |\n|fare_amount | The time-and-distance fare calculated by the meter |\n|total_amount | The total amount charged to passengers. Does not include cash tips |\n\n###Now, we need to convert from *Dataset[org.apache.spark.sql.Row]* to *Dataset[our class]*.\n\n###We are going to create our class named *Trip* to store the fields that we need.  \n"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"7E38AAFF21ED499DACE305653681A3D8"},"cell_type":"code","source":"case class Trip(\n  pickupTime: Long,\n  dropoffTime: Long,\n  passengerCount: Int,\n  tripDistance: Double,\n  pickupX: Double,\n  pickupY: Double,\n  dropoffX: Double,\n  dropoffY: Double,\n  totalAmount: Double\n)","outputs":[]},{"metadata":{"id":"5BC9B6CAC4D34D6283651B56F1C78E49"},"cell_type":"markdown","source":"###In class [Row](https://spark.apache.org/docs/2.2.0/api/java/org/apache/spark/sql/Row.html), we have the method *Row:fieldIndex(String)* that returns the index of a given field name.\n\n###We are going to use *Row:fieldIndex(String)* to get the fields we want to.\n"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"506117D725604E368E98BA68A2A36AE9"},"cell_type":"code","source":"import org.apache.spark.sql.Row\n\nclass RichRow(row: Row) {\n  def getAs[T](field: String): Option[T] = {\n    if (row.isNullAt(row.fieldIndex(field))) {\n      None\n    } else {\n      Some(row.getAs[T](field))\n    }\n  }\n}","outputs":[]},{"metadata":{"id":"06931020AFB541289FFD8834999955BC"},"cell_type":"markdown","source":"###We have date/time fields: *tpep_pickup_datetime* and *tpep_dropoff_datetime*\n\n###We need a function to convert it from String \"yyyy-MM-dd HH:mm:ss\" to Long unix epoch milliseconds "},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"F9851B6626914E03878BF8F6F92B29D7"},"cell_type":"code","source":"import java.time.format.DateTimeFormatter\nimport java.time.temporal.ChronoField\nimport java.time.Instant\n\ndef stringToUnixEpochMilliseconds(rr: RichRow, timeField: String): Long = {\n    val formatter = DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss Z\")\n    val optDt = rr.getAs[String](timeField) \n    optDt.map(dt => Instant.from(formatter.parse(dt + \" -0300\")).toEpochMilli).getOrElse(0L)\n}","outputs":[]},{"metadata":{"id":"29E2B3DE0EF74FCA9F2CCD95D2D9165F"},"cell_type":"markdown","source":"###We have some fields that need to be converted from String to Double.\n\n###We need a function for that."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"8B0AB64EC3B247E9871876B2A8E3965A"},"cell_type":"code","source":"def stringToDouble(rr: RichRow, locField: String): Double = {\n  rr.getAs[String](locField).map(_.toDouble).getOrElse(0.0)\n}","outputs":[]},{"metadata":{"id":"70ED721356AE42B7B542691DDB349034"},"cell_type":"markdown","source":"###We have some fields that need to be converted from String to Int.\n\n###We need a function for that."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"EB69CD6B04224F9E863FC9EB2579A037"},"cell_type":"code","source":"def stringToInt(rr: RichRow, locField: String): Int = {\n  rr.getAs[String](locField).map(_.toInt).getOrElse(0)\n}","outputs":[]},{"metadata":{"id":"70321277459643FB80D0C3B023C3B103"},"cell_type":"markdown","source":"###Using the functions *stringToUnixEpochMilliseconds*, *stringToInt* and *stringToDouble* we are going to create a function to convert each row of the DataFrame to our Trip object."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"221F9D6D7ED54F29B8670DC1BF946B11"},"cell_type":"code","source":"def parse(row: Row): Trip = {\n  val rr = new RichRow(row)\n  Trip(\n    pickupTime = stringToUnixEpochMilliseconds(rr, \"tpep_pickup_datetime\"),\n    dropoffTime = stringToUnixEpochMilliseconds(rr, \"tpep_dropoff_datetime\"),\n    passengerCount = stringToInt(rr, \"passenger_count\"),\n    tripDistance = stringToDouble(rr, \"trip_distance\"),\n    pickupX = stringToDouble(rr, \"pickup_longitude\"),\n    pickupY = stringToDouble(rr, \"pickup_latitude\"),\n    dropoffX = stringToDouble(rr, \"dropoff_longitude\"),\n    dropoffY = stringToDouble(rr, \"dropoff_latitude\"),\n    totalAmount = stringToDouble(rr, \"total_amount\")\n  )\n}\n","outputs":[]},{"metadata":{"id":"CA2BBF6B71424212821FFCE1A1AF3020"},"cell_type":"markdown","source":"###Let's test our function on a sample of our data."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab283232256-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"B8477139277F45568891F9E2473E4C6A"},"cell_type":"code","source":"val taxiRawSample = taxiRaw.sample(false, 0.0001d)\nval taxiRawSampleTrip = taxiRawSample.map(parse)\ntaxiRawSampleTrip.take(4)","outputs":[]},{"metadata":{"id":"155E684D65CC4221A7A2EA99015D8958"},"cell_type":"markdown","source":"###Works!!\n\n###As you can see, taxiRawSample is *Dataset[Row]*.  \n\n###After transformation, taxiRawSampleTrip is *Dataset[Trip]*.\n  \n  "},{"metadata":{"id":"DE07218FD86E4E4C9E1F22722948D7DD"},"cell_type":"markdown","source":"###In a big dataset, we need to handle invalid records.\n\n###It will either parse the record successfully and return meaningful output, or it will fail and throw an exception, in which case we want to capture both the value of the invalid record and the exception that was thrown. \n\n###Whenever an operation has two mutually exclusive outcomes, we can use Scala’s Either[L, R] type to represent the return type of the operation. \n\n###For us, the “left” outcome is the successfully parsed record and the “right” outcome is a tuple of the exception we hit and the input record that caused it."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"F08CBDB1C1D543B887766B26522B09D5"},"cell_type":"code","source":"def safe[S, T](f: S => T): S => Either[T, (S, Exception)] = {\n  new Function[S, Either[T, (S, Exception)]] with Serializable {\n    def apply(s: S): Either[T, (S, Exception)] = {\n      try {\n        Left(f(s))\n      } catch {\n        case e: Exception => Right((s, e))\n      }\n    }\n  }\n}","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"9279EF865911429CB1B636537F3A4B9D"},"cell_type":"code","source":"val safeParse = safe(parse)\nval taxiParsed = taxiRaw.rdd.map(safeParse)","outputs":[]},{"metadata":{"id":"21F3D44598024E0283352818EF91D5E4"},"cell_type":"markdown","source":"###Let's count how many successes and failures we have.\n"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"88A7C6404E1346C18F9C170CF26DB5F5"},"cell_type":"code","source":"taxiParsed.map(_.isLeft).countByValue().foreach(println)","outputs":[]},{"metadata":{"id":"651ABF542C814A6AAA7B70032496A6B1"},"cell_type":"markdown","source":"###Great!!! \n\n###What luck!!! none of the records threw exceptions during parsing!\n\n###Let's parse everything and cache it."},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"BB9D10004B3E442183C9D51E1F01B625"},"cell_type":"code","source":"val taxiGood = taxiParsed.map(_.left.get).toDS\ntaxiGood.cache()","outputs":[]},{"metadata":{"id":"E3862AA1000C42248BCCC70F26D68528"},"cell_type":"markdown","source":"###Wait a minute.  \n\n###We are not sure that for each trip dropoff time  < pickup time. And we expected that a trip does not take more than few hours, although it’s certainly possible that long trips, trips that take place during rush hour, or trips that are delayed by accidents could go on for several hours. \n\n### We need to check it.\n\n###Let's define a helper function to check that dropoff time > pickup time and how many hours a trip takes"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"999A8ED581074CE8A0934B9C87E02E5E"},"cell_type":"code","source":"import java.util.concurrent.TimeUnit\n\nval hours = (pickup: Long, dropoff: Long) => {\n  TimeUnit.HOURS.convert(dropoff - pickup, TimeUnit.MILLISECONDS)\n}","outputs":[]},{"metadata":{"id":"B66353DD45184FEF8CFE6456D4D51D1D"},"cell_type":"markdown","source":"###Let's test the function\n\n### pickup time = 0 and dropoff time = 1"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"E76706448230401DA8A2B353E98A2434"},"cell_type":"code","source":"hours(1,0)","outputs":[]},{"metadata":{"id":"E7773331E79F44A8B5C6B6FDB4882E08"},"cell_type":"markdown","source":"###Does not work as we expected\n\n###Let's redefine"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"325AE43C4E9D47BB87FA79F50D080F9D"},"cell_type":"code","source":"val hours = (pickup: Long, dropoff: Long) => {\n  val retVal = TimeUnit.HOURS.convert(dropoff - pickup, TimeUnit.MILLISECONDS)\n  if (dropoff - pickup < 0 && retVal == 0) {\n    -1\n  } else {\n    retVal\n  }\n}","outputs":[]},{"metadata":{"id":"89E7ACCACAD14CB1833496241CD6A9D0"},"cell_type":"markdown","source":"###Test again"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"3D89CE3D921049EE8A51CC33DA4D0695"},"cell_type":"code","source":"hours(1,0)","outputs":[]},{"metadata":{"id":"516106D221E44C2B896E80D1C7717F33"},"cell_type":"markdown","source":"###Great, worked!!!"},{"metadata":{"id":"321379C9E351462C8B79132C4A5858FD"},"cell_type":"markdown","source":"###We would like to be able to use our hours function to compute a histogram of the number of trips that lasted at least a given number of hours.\n\n###This sort of calculation is exactly what the Dataset API and Spark SQL are designed to do.\n\n###We can only use these methods on the columns of a Dataset instance, whereas the hour UDF is computed from two of these columns.\n"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab2068329280-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"37EC286D029947D38BF0DF9F6E81B0F1"},"cell_type":"code","source":"import org.apache.spark.sql.functions.udf\n\nval hoursUDF = udf(hours)","outputs":[]},{"metadata":{"id":"F8F0E273981147358AF33F16BDC44571"},"cell_type":"markdown","source":"###Let's take a look at the hours\n"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab1872543837-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"4022B10A485040E0BF4033B406CC657B"},"cell_type":"code","source":"taxiGood.\n  groupBy(hoursUDF($\"pickupTime\", $\"dropoffTime\").as(\"hours\")).\n  count().\n  sort(\"hours\").\n  collect()","outputs":[]},{"metadata":{"id":"7949766BF91647DD808413737D2DAAA8"},"cell_type":"markdown","source":"###Wait!! \n\n###We have trips with negative hours. Perhaps the DeLorean from Back to the Future is moonlighting as an NYC taxi.\n\n###Most of the trips have hours between 0 and 1 hours. Because of that the histogram does not help.\n\n###Let's check the trips with hours bigger than one hour\n"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab861189863-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"8330630F23C64D678F27C00CE3A9D1C1"},"cell_type":"code","source":"taxiGood.\n  groupBy(hoursUDF($\"pickupTime\", $\"dropoffTime\").as(\"hours\")).\n  count().\n  where($\"hours\" > 1).\n  sort(\"hours\").\n  collect()\n","outputs":[]},{"metadata":{"id":"2057AF9368184CA287E4437E178AF9FF"},"cell_type":"markdown","source":"###Let's look the negative hours"},{"metadata":{"trusted":true,"input_collapsed":false,"output_stream_collapsed":true,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab16230736-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"E3D9EAF54ED2489C841A862FC0700C5A"},"cell_type":"code","source":"taxiGood.\n  groupBy(hoursUDF($\"pickupTime\", $\"dropoffTime\").as(\"hours\")).\n  count().\n  where($\"hours\" < 0).\n  sort(\"hours\").\n  collect()","outputs":[]},{"metadata":{"id":"366210EB8BC2415AB9D17E19DCC9687C"},"cell_type":"markdown","source":"###For sure, we are going to remove the trips with negative hour\n\n###But, the positive values. How could we see if the record is okay?\n\n###We can verify that by analyzing the mean of trip distance and total amount for each hour"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab611361578-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"52801167E97640418C7F7B97B53EB186"},"cell_type":"code","source":"taxiGood.\n  groupBy(hoursUDF($\"pickupTime\", $\"dropoffTime\").as(\"hours\")).\n  agg(mean($\"totalAmount\"), mean($\"tripDistance\")).\n  sort(\"hours\").\n  collect()","outputs":[]},{"metadata":{"id":"64C1E69C8237471786CB643B0E4E8B56"},"cell_type":"markdown","source":"###Starting at hour == 0, we notice that the mean of trip distance and total amount are increasing until hour == 2\n\n###At hour == 2, the mean of trip distance and total amount are almost the same as hour == 1\n\n###Based on the mean of trip distance and total amount, we are going to remove the trips where the hour >= 0 and hours <= 2 because it does not make sense the hour is increasing but the  trip distance and total amount are not increasing\n\n###So let's filter it"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab1480432268-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"5E687142BF2440ED8A8E4189DE58872A"},"cell_type":"code","source":"//  Does not work in spark-notebook but works in spark-shell\n// spark.udf.register(\"hours\", hours)\n// val taxiClean = taxiGood.where(\n//   \"hours(pickupTime, dropoffTime) BETWEEN 0 AND 3\"\n// )\n\nval taxiClean = taxiGood\n  .filter(trip => {\n    val hour = hours(trip.pickupTime, trip.dropoffTime)\n    hour >= 0 && hour <= 2\n  })\n\ntaxiClean.collect","outputs":[]},{"metadata":{"id":"ADA45C2DF4B64B43880D54726EDCD6C1"},"cell_type":"markdown","source":"###Now we are going to take a look at the coordinates\n\n###Let's see if we have fields with weird position, like 0"},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"presentation":{"tabs_state":"{\n  \"tab_id\": \"#tab354135281-0\"\n}","pivot_chart_state":"{\n  \"hiddenAttributes\": [],\n  \"menuLimit\": 200,\n  \"cols\": [],\n  \"rows\": [],\n  \"vals\": [],\n  \"exclusions\": {},\n  \"inclusions\": {},\n  \"unusedAttrsVertical\": 85,\n  \"autoSortUnusedAttrs\": false,\n  \"inclusionsInfo\": {},\n  \"aggregatorName\": \"Count\",\n  \"rendererName\": \"Table\"\n}"},"id":"A16E471593804BFCA745DA0F4265A9B6"},"cell_type":"code","source":"val taxiCleanXY = taxiClean.filter(trip => trip.pickupX != 0 && trip.pickupY != 0 && trip.dropoffX != 0 && trip.dropoffY != 0)\ntaxiCleanXY.collect","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"148EB44DC20E4D0A8D0F45BD5F73E743"},"cell_type":"code","source":"case class Heatmap(\n  dy: Int,\n  ts: Long,\n  ln: Double,\n  lt: Double,\n  ct: Int\n)","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"98855B9E98904A3EB8127B63A71F1115"},"cell_type":"code","source":"var taxiCleanPickup = taxiCleanXY.map(trip => Heatmap(Instant.ofEpochMilli(trip.pickupTime).atZone(java.time.ZoneId.of(\"UTC-0300\")).getDayOfYear, trip.pickupTime, trip.pickupX, trip.pickupY, 1))\nval sessionsPickup = taxiCleanPickup.repartition($\"dy\").sortWithinPartitions($\"dy\", $\"ts\")\nsessionsPickup.write.json(path + \"sessionsPickup\")\n","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false,"id":"99C3F400DA3F4B1B899ACCF57587432B"},"cell_type":"code","source":"var taxiCleanDropoff = taxiCleanXY.map(trip => Heatmap(Instant.ofEpochMilli(trip.dropoffTime).atZone(java.time.ZoneId.of(\"UTC-0300\")).getDayOfYear, trip.dropoffTime, trip.dropoffX, trip.dropoffY, 1))\nval sessionsDropoff = taxiCleanDropoff.repartition($\"dy\").sortWithinPartitions($\"dy\", $\"ts\")\nsessionsDropoff.write.json(path + \"sessionsDropoff\")\n","outputs":[]}],"nbformat":4}